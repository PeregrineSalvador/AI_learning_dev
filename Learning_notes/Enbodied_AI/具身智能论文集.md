# 具身智能基础技术论文

> 具身智能一定是系统化的工程，每一个部分都涉及到一个或几个模型去应用
## 场景理解

> 发展的最早，也是最成熟的一个部分。这一部分我的感觉是可以直接上手实践
>
> 足量的实践即可替代知识的空缺

> Grounding是对齐的含义

##### 检测分割

> 传统视觉和深度学习的结合，做的是一些基础任务

* SAM	Open-Voc Detection
* SAM3D    Open-Voc Detecion in Point Cloud

##### 多模态 Grounding

* pix Grounding

## 数据引导

> 这里说的是数据引导而不是数据收集，因为现阶段基于数据收集的方式进行机器人数据集积累是远远达不到训练的要求
> 因而希望采用一种数据引导的方式，用开放世界中已经有的视频或者别的形式来训练机器人进行学习
##### 视频学习 Video_Learning

> 完全脱离硬件平台，通过对收集视频里面人的行为来做数据收集
>
> 成本极低，但是难度大，效果差，不是主流路线

* MimicPlay
* Vid2Robot

##### 硬件在环采集 Light_Hardware

> 轻量硬件平台进行数据收集

* UMI 类似松灵的Pika
* DexCap 视觉+自制手套捕获数据训练机械臂+灵巧手 就是之前总结过的VR遥操作方案
* HIRO Hand 用硬件转换和手套，不需要进行额外转换

> 全面仿真or重量级硬件平台进行数据收集
>
> 这里很重视灵巧手的数据采集 

* VR显示器+VR手套 Tesla方案

##### 生成式仿真 Gen_Sim

* RoboGen
* MimicGen

## 动作执行

##### 生成式模仿学习

* ALOHA

* Diffusion Policy

##### AORDANCE

* roboaffordance 机器人学习在区域内如何操作 但是比较基础和简单，是找到一个可接触的像素点之后，然后直接执行

* affordPose 相对复杂，是针对灵巧手的，不仅是需要明白要抓取什么，还要学到有多少种抓取方案要做，还有每一个部分具体有什么作用
* SceneFun3D 更加复杂，是针对开放世界的识别与如何进行抓取

##### 大语言模型的问答

> Prompt Planning from LLM

* ManipLLM 通过大语言模型的问答，然后获得具体的坐标信息。更多是利用了LLM的多模态理解和复杂规划能力
* ManipVQA

##### language校正

> 如何介入人的指令来进行任务执行的校正

* OLAF
* Yell At your Robot

## 世界模型

* 3D VLA
* LAPO

# 目前的问题

### 机器人大脑和小脑发展不平衡

* 机器人小脑的灵巧操作发展远远落后于大脑的思维决策过程

> 客观上数据收集量太少，并且发展的比较晚
>
> 技术路线目前还没有同意
