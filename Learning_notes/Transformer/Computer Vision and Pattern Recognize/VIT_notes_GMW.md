# VIT个人解读

本文将对 Vision Transformer (ViT) 的核心实现进个人的解读，结合代码分析其设计思想和实现细节。ViT 作为CV领域的革命性模型，成功将自然语言处理中的 Transformer 架构引入视觉任务，实现了纯注意力机制对卷积的替代。

## 注意力机制作为视觉基础

ViT 的核心创新在于摒弃了传统 CNN 的**归纳偏置**，转而使用**自注意力机制**处理图像信息。这是由多方面决定的，一方面是图像数据本身就可以视作是序列数据（通过PatchEmbedding），另一方面是图像不光是像在CNN的归纳偏置那样具有局部性，同时，图像也是拥有全局性的。当然，最重要的还是笔者在上次笔记提到的，自注意力机制异常强大。

## 模型架构详解

### PatchEmbedding

Transformer架构最初为自然语言处理设计，其输入是一维的单词序列。图像则是二维/三维的像素矩阵，无法直接输入。PatchEmbedding的核心作用就是将二维图像“转换”或“重塑”成一维的序列（Sequence），从而使Transformer能够处理视觉数据。

PatchEmbedding的实现主要包含两个关键步骤：

#### 1. 图像分块 (Patching)

将一张输入图像划分成多个大小固定、互不重叠的小方块，每个小方块称为一个 **Patch**（或图像块）

#### 2. 线性投影 (Linear Projection)

将每个Patch拉平（Flatten）成一个一维向量，并通过一个可学习的**线性投影层**（通常是全连接层或卷积层）映射到一个指定的、更高维的向量空间。这个投影过程同时也是特征提取的过程。

***请注意： 在和万鹏学长的交谈中，由于本人为查明清楚，实际上，这个线性层全连接层或卷积层都是可以的，我当时错误地说成仅由卷积层实现，当然，实际上，卷积层是 ViT 论文原版和绝大多数实现中采用的方法，非常巧妙和高效。下面两种的具体实现我会简明的讲一讲，如有不足请指正。***

**卷积实现**

我们给予pytorch框架设计神经网络是高度集成的，所以实际上，卷积实现在代码实现上是极其简单的：

```python
self.projection = nn.Conv2d(in_channels=3, out_channels=768, kernel_size=16, stride=16)
```

笔者认为这里有必要稍微复习一下卷积核这里的具体操作（是因为笔者自己有点忘了，两位学长肯定很扎实（狗头））

**输入**：图像张量，形状为 `[B, 3, 224, 224]`。

**卷积操作**：

卷积核在图像上滑动，**每次移动的步长为 16**，因此卷积核的每个“滑动位置”刚好对应一个 **16x16 的 Patch**。

卷积核的**输出通道数设置为 768**。这意味着有 **768 个不同的卷积核**，每个都会对输入 Patch 进行卷积操作。

经过卷积后，输出形状变为 `[B, 768, 14, 14]`。这里的 `14` 来源于 `224 / 16`，代表图像被划分成了 **14x14 的网格**，每个网格位置对应原始图像中的一个 Patch。

此时，**输出张量中的每个 `[i, :, j, k]` 可以理解为第 `(j,k)` 个 Patch 被投影后的 768 维嵌入向量**。你可以将这 768 个通道的、在 `(j,k)` 位置的值，想象成是同一个 Patch 被 768 个不同的“特征检测器”处理后的结果。

在得到这个三维矩阵后进行下面的展平操作即可，使用 `x.transpose(1, 2)` 将序列长度维度 (`196`) 和特征维度 (`768`) 交换，得到最终的 Patch Embedding 序列 `[B, 196, 768]`。这符合 Transformer 的输入要求（批次大小, 序列长度, 特征维度），没啥东西。

**全连接层操作：**

先将图像手动分割成多个 Patch**，将每个 Patch**展平**成一个一维向量，然后**通过一个全连接层**进行线性投影即可。

大家都知道卷积在局部特征的处理天然地比顾及全局但是笨重的全连接层要高校的多，这也是这里使用卷积处理而非fc处理的核心原因。

###  TransformerEncoderBlock 

简单来说，这个架构核心功能是通过**自注意力机制 (Self-Attention)** 和**前馈神经网络 (Feed-Forward Network, FFN)** （这个就是MLP）来提炼和增强输入序列的特征表示，并辅以**残差连接 (Residual Connection)** 和**层归一化 (Layer Normalization)** 来保障训练的稳定性。

```python
class TransformerEncoderBlock(nn.Module):
    def __init__(self, dim, heads, mlp_dim, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attention = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        attn_output, _ = self.attention(self.norm1(x), self.norm1(x), self.norm1(x))
        x = x + attn_output
        mlp_output = self.mlp(self.norm2(x))
        x = x + mlp_output
        return x
```

处理好的序列数据自然是要转换为富含上下文信息的表示。

先是多头自注意力子层。自注意力机制允许序列中的每个位置（token）关注到序列中的所有其他位置，从而捕获序列内部的依赖关系，距离无关。具体而言，对于每个词元，模型会计算其查询（Query）、键（Key）和值（Value）向量。注意力分数的计算是通过将查询向量与所有键向量进行点积，然后缩放并应用 Softmax 函数得到权重，最后用这些权重对值向量进行加权求和，为每个词元生成一个融合了全局上下文信息的新表示。所谓“多头”注意力，是指将上述过程独立地重复多次（例如8头），每次使用不同的可学习参数矩阵将输入投影到不同的子空间（每个头的维度为 d_k = d_model / num_heads）。这使得模型能够并行地捕捉多种不同类型的依赖关系（例如语法关系、语义关系等）。每个头产生的上下文向量被拼接起来，最后再通过一个可学习的线性投影矩阵整合回 d_model 维度的输出。
在多头自注意力子层之后，模型会立即执行一次**残差连接**（Residual Connection）和**层归一化**（Layer Normalization）。**残差连接是将子层的输入（即多头注意力计算前的值）直接与其输出相加。这有助于缓解深度网络中的梯度消失问题，使得模型能够训练更深的网络。随后进行的层归一化是对相加后的结果进行规范化，它针对单个样本的所有特征维度计算均值和方差并进行缩放，有助于稳定训练过程并加速收敛。**这与批量归一化（Batch Normalization）不同，后者是在批次维度上进行归一化，对于序列长度可变的任务，层归一化通常表现更佳。

经过第一个残差连接和层归一化后，数据会流入第二个子层：基于位置的**前馈神经网络**（Positionwise Feed-Forward Network，FFN）。这是一个应用于序列中每个位置（词元）的独立的**多层感知机**（MLP），之所以称为“基于位置”，是因为它对序列中的每个位置表示进行变换时使用的是同一个MLP。其典型结构是两个线性变换层，中间夹着一个ReLU激活函数：FFN(x) = max(0, xW₁ + b₁)W₂ + b₂。第一个线性层通常会将维度从 d_model 扩大到一个更大的中间维度（例如2048），第二个线性层再将其投影回 d_model 维度。这个前馈网络的作用是为模型提供非线性变换能力，进一步处理和增强特征表示。

同样地，在前馈神经网络子层之后，也会再次执行一次残差连接和层归一化。前馈网络的输出与其输入（即第一个AddNorm层的输出）相加，然后再进行一次层归一化，最终得到一个编码器层的输出17。上述过程会在编码器中重复多次，通常由 N 个这样的编码器层堆叠而成（N 通常为6层或更多）。每一层都接收前一层的输出，并逐步提炼和抽象序列的表示。通过这种堆叠，模型能够构建出越来越复杂和深层的序列表征，每一层都可能捕获到不同层次或不同方面的上下文信息。

到此这个VIT的主要部分就都讲完了，一如既往的调用组件，训练，预测，当然VIT不能忘了cls_tokens操作，位置编码操作，再搞点正则化等等来优化，这些就不再赘述了，可以参考我上传的代码。

谢谢看到这里，显然，你是一个有耐心的人，你做什么都会成功的!
